* Case Studies
** The Assignment Problem
The first case study we'll examine is an implementation of a solution to the
assignment problem based off the Hungarian algorithm with correctness guarantees
maintained throughout the steps of the algorithm.

*** Problem Definition
The assignment problem is a classic example of a combinatorial optimisation
problem. The problem involves assigning a number of /agents/ to /tasks/. An agent
can be assigned to any task however there is a cost to assigning a particular
agent to a particular task. The assignment problem is the problem of assigning
exactly one task to each agent so that the total cost of doing this assignment
is minimal.

To think of the problem in a concrete setting we might imagine a clinical
practice with patients that need treatments as our tasks. The agents in this
scenario are the doctors that will be assigned to treat patients. Doctors can
perform the treatment for any of the patients but a doctor may be suited more
towards a specific kind of treatment. The assignment problem in this setting is
finding a way to assign the doctors to treat the patients in a way that
optimally assigns the doctors based on their ability to perform specific
treatments.

*** Existing Algorithms
It may seem that the time complexity of an algorithm to solve such a problem
would require time in the order $O(n!)$ time with respect to the number of
agents/tasks. Optimisation strategies exist however that produce polynomial time
algorithms. The most famous example is the Hungarian algorithm which runs in
$O(n^4)$ time. Further algorithmic optimisations have been applied such as James
Munkres' proven algorithm cite:munkres_assignment_1957 that demonstrates
$O(n^3)$ time complexity.

*** Hungarian Algorithm
Our Idris implementation focuses on implementing the Hungarian algorithm due to
its straight forward specification. There are also a number of correctness
properties that jump off the page when reading the specification that we can
attempt to write proofs for and in turn create a verified implementation that
follows the steps properly. The Hungarian algorithm frames the problem in terms
of a matrix that represents the tasks, agents and the weighting between them.

The steps of the algorithm are as follows:
1. Find the minimum value in each row and subtract it from all other values in
   its row.
2. Find the minimum value in each column and subtract it from all other values
   in its column.
3. Cover each 0 in the matrix with a line such that the minimal number of lines
   is used.
4. Find the smallest value in the cost matrix not covered by a line. Subtract it
   from each uncovered row. Add it to each covered column. Return to step 3.

We determine if we have completed an assignment following each step. For step 1
this means checking if there is a 0 in each column. For the steps after step 1
we check if we have created a minimal covering. If the minimum number of
covering lines required is equal to the size of our square matrix then a minimal
assignment has been found and the algorithm has been completed.

The approach taken in this case study was to provide three different versions of
the algorithm, each one providing more correctness guarantees than the previous.
In order to accomplish this within the time constraints of the project only the
first two steps of the algorithm are being considered. However there are
interesting correctness properties that we can discuss and also some interesting
design decisions made in terms of approach and design of our API.

*** Demonstration
First let us assume that we have some initial matrix that describes our tasks
and our agents that we would like to create an assignment for.

\[ \begin{bmatrix}
8 & 20 & 12 \\
3 & 2 & 14 \\
9 & 8 & 4
\end{bmatrix} \]

When applying step 1 we choose 8 in the first row, 2 in the second row and 4 in
the last row. We then subtract these values across their respective rows. This
results in the transformed matrix below.

\[ \begin{bmatrix}
\textcolor{red}{8} & 20 & 12 \\
\textcolor{red}{2} & 3 & 14 \\
9 & 8 & \textcolor{red}{4}
\end{bmatrix} \rightarrow
\begin{bmatrix}
0 & 12 & 4 \\
0 & 1 & 12 \\
5 & 4 & 0
\end{bmatrix} \]

As we can see, there are zeroes in both the first and third columns but there is
not a zero in the second column. We have yet to create a minimal assignment so
we proceed to apply step 2 to the transformed matrix that resulted from applying
the first step.

\[ \begin{bmatrix}
\textcolor{red}{0} & 12 & 4 \\
0 & \textcolor{red}{1} & 12 \\
5 & 4 & \textcolor{red}{0}
\end{bmatrix} \rightarrow
\begin{bmatrix}
0 & 11 & 4 \\
0 & 0 & 12 \\
5 & 3 & 0
\end{bmatrix} \]

When we have completed step 2 we now create a set of lines that will cover all
of the zeroes in the matrix. As we can see we have had to draw a minimum of 3
lines to cover the zeroes in the matrix at this point.

\[
\stackinset{c}{}{c}{1.0\baselineskip}{\rule{2.8\baselineskip}{.5pt}}{%
\stackinset{c}{}{c}{0.01\baselineskip}{\rule{2.8\baselineskip}{.5pt}}{%
\stackinset{c}{1.05\baselineskip}{c}{}{\rule{.5pt}{2.7\baselineskip}}{%
\begin{bmatrix}
  0 & 11 & 4 \\
  0 & 0 & 12 \\
  5 & 3 & 0
\end{bmatrix}}}}
\]

We have created a minimal covering with the number of lines equal to the size of
our matrix. The algorithm tells us that we have found a minimal assignment at
this point and we can stop. In this case, the first agent is assigned task 1,
the second agent is assigned task 2 and the third agent is assigned task 3.

*** Invariants
There are some interesting invariants we notice when performing these steps.
After performing the first step we can be certain that at least one of the
values in each row will be a zero. The minimum value that we find must be an
element of the row. At some point when subtracting it across the row we will
subtract it from itself. This will result in at least one zero.

*Step 1*:
- Precondition - A non-empty matrix
- Postcondition - A non-empty matrix where all rows contain at least one zero

Similarly we can say that after completing the second step there will be at
least one zero in each row and at least one zero in each column. This follows
from the same reasoning as before because the element we subtract from the
column will be an element of that column.

*Step 2*:
- Precondition - A non-empty matrix where all rows contain at least one zero
- Postcondition - A non-empty matrix where all rows contain at least one zero
  and all columns contain at least one zero

These invariants will be studied in more detail as we outline the development of
the different Idris implementations of the algorithm.

*** First Implementation - Lists
The first implementation uses the standard list type. As we will see, this
implementation demonstrates the fewest number of correctness guarantees with no
proofs of the invariants outlined given and introduces partiality and the risk
of runtime errors that come with partial functions. This implementation is quite
similar to how you might approach the problem in a language such as Haskell with
limited information at the type level.

In order to implement the algorithm we first need to decide how the data is
modelled. In this list implementation our matrix will be defined as a list of
lists of ints. We can use Idris' ability to calculate types as the results of
functions to create some type aliases that allow us to write more specific types
that relate to the domain of the algorithm. In this case, =HungarianMatrix= as
opposed to =List (List Int))=.

#+CAPTION: Type aliases to represent our cost matrix
#+BEGIN_SRC idris
Matrix : Type -> Type
Matrix a = List (List a)

HungarianMatrix : Type
HungarianMatrix = Matrix Int
#+END_SRC

As both the first step and the second step of the algorithm require that we find
the minimum of rows and columns respectively we will need to define a function
that finds the minimum of a list that contains elements with some notion of ordering.

#+CAPTION: The =minimum= function defined over lists
#+BEGIN_SRC idris
listMin' : Ord a => a -> List a -> a
listMin' x [] = x
listMin' x (y :: ys) with (compare x y)
  listMin' x (y :: ys) | GT = listMin' y ys
  listMin' x (y :: ys) | EQ = listMin' y ys
  listMin' x (y :: ys) | LT = listMin' x ys

partial
listMin : Ord a => List a -> a
listMin (x :: xs) = listMin' x xs
#+END_SRC

In this function we've made use of the interface mechanism of Idris. This system
is analogous to the type class system in Haskell. We can define the =listMin=
function using ad-hoc polymorphism over any type that provides an implementation
of the =Ord= interface.

Despite this function working as intended and type-checking, a problem is
starting to emerge. The =listMin= function is a partial function. There is no
minimum value that we can get from an empty list. If the provided matrix is
empty then we will receive an error at runtime and crash. The compiler has been
able to catch this function if we had left out the =partial= annotation due to the
use of the =%default total= compiler pragma in our implementations.

One way we might solve this problem is by taking a default value as an extra
argument. This however does not make sense semantically as it is not the minimum
value of the passed list. We will have to accept when using lists that we may
have an empty case and as such our =listMin= function is partial.

The effect of this partial function at the centre of this implementation of the
algorithm is that it has a chain reaction on the totality of the rest of the
functions required to implement the algorithm. We now consider the effect of
this partiality on our =subSmallest= function that performs the step of
subtracting our minimum values across all rows/columns in our matrix.

#+CAPTION: Subtracting the minimum values across the matrix
#+BEGIN_SRC idris
partial
subSmallest : HungarianMatrix -> HungarianMatrix
subSmallest [] = []
subSmallest (x :: xs) = map (flip (-) $ (listMin x)) x :: subSmallest xs
#+END_SRC

As defined, =subSmallest= matches both cases, the empty list case and the
non-empty case. Then we might ask why does this function have an annotation that
it is partial? The reason is that in the second function clause we make a call
to the =listMin= function. As this function is partial this has the knock-on
effect of introducing partiality into the =subSmallest= function which is used
throughout the algorithm. If we were to remove this partiality annotation the
compiler will correctly inform us that the function has failed to satisfy the
totality checker.

#+BEGIN_EXAMPLE
idris> :total subSmallest
HungarianList.subSmallest is possibly not total due to:
    HungarianList.listMin, which is not total as there are missing cases
#+END_EXAMPLE

This partiality bubbles up to the top level of the algorithm at the point where
we export the function that end users will use. Any consumer of this algorithm
should be prepared to either ensure that they never provide an empty matrix to
the =hungarianMethod= function or they risk that their program crashes due to an
unmatched pattern in the =listMin= function.

Obviously we would like to do better than that and not introduce ways to crash
code calling into our algorithm. These kinds of errors should be caught
statically at compile time. Fortunately Idris provides the tools to ensure that
these kinds of errors are made impossible through sufficiently descriptive
types. This is the avenue we will explore and demonstrate with the next
implementation.

*** Second Implementation - Vectors
In the second implementation we begin to narrow down the possibilities of
incorrect type-checking functions. To start off with, the definition of our
=HungarianMatrix= type has been modified. We make use of the Idris standard
library type =Matrix n m a= which is simply defined as =Vect n (Vect m a)=.

#+CAPTION: Type alias to represent our cost matrix
#+BEGIN_SRC idris
HungarianMatrix : (n : Nat) -> {auto p : n `GT` Z} -> Type
HungarianMatrix Z {p = LTEZero} impossible
HungarianMatrix Z {p = (LTESucc _)} impossible
HungarianMatrix (S k) {p = (LTESucc x)} = Matrix (S k) (S k) Int
#+END_SRC

There are a couple of things worth noting in this definition. The aim of this
type alias is to ensure that our =HungarianMatrix= can only represent the square
matrices that have at least 1 row and column. The first argument to this
type-computing function is the size $n$ of our matrix.

The second argument represents a proof that $n > 0$. The curly braces
surrounding this argument denote that it is an implicit argument as outlined in
the previous chapter. As it is implicit, we do not need to explicitly pass this
proof when calling this function to create the type. For example, we can refer
to a square matrix of size 2 as having the type =HungarianMatrix (S (S Z))=
rather than =HungarianMatrix (S (S Z)) someProofTerm=. The use of the =auto=
keyword allows the compiler to perform a proof search to find the proof term
that fits instead of forcing us as the user to create it.

We can bring the implicit argument p down to our function definition and perform
a case split on it. We know that if the natural number passed to the function is
ever zero then it is impossible to have a value of type =Z `GT` Z= and as such
the compiler accepts that the two clauses above are impossible. This definition
was carried out in a type-driven way by defining our initial specification that
$n$ had to be greater than zero (i.e. we wanted to create a non-empty matrix).
By using the interactive editing capabilities of Idris and case splitting on the
proof term, the clauses above fell out immediately leaving just one clause that
wasn't impossible and where we could insert the definition of our non-empty
matrix.

Using the =Vect= type of length-indexed lists helps us achieve more totality in
this implementation. In our previous list of lists based version the partiality
of our =listMin= function had the effect of bubbling up and creating partiality in
all of the functions in our interface. Using the =Vect= type of length-indexed
lists we can reduce the number of cases that need to be matched in order to
achieve a total definition of a minimum function. Restricting our type to only
work on vectors that have at least one element ensures that we can always return
a value from the =vectMin= function. If we tried to pattern match on the nil
constructor for =Vect= then that would result in a type error as we've explicitly
stated in the type of =vectMin= that it will only accept vectors with at least one
element as input.

#+CAPTION: The =minimum= function defined over length-indexed lists
#+BEGIN_SRC idris
vectMin' : Ord a => a -> Vect n a -> a
vectMin' x [] = x
vectMin' x (y :: ys) with (compare x y)
  vectMin' x (y :: ys) | GT = vectMin' y ys
  vectMin' x (y :: ys) | EQ = vectMin' y ys
  vectMin' x (y :: ys) | LT = vectMin' x ys

vectMin : Ord a => Vect (S n) a -> a
vectMin (x :: xs) = vectMin' x xs
#+END_SRC

The effect that this has on our code is that now if a consumer of our algorithm
tries to call into it with an empty matrix that error can be statically detected
at compile time. We don't run the risk of introducing runtime errors that may
only be detected when a system has been running for days with buggy and
under-specified code. This reflects itself in the types of the functions that
implement the specific steps in the algorithm. The types outlined below provide
us with the knowledge that we can never call these functions with ill-typed data
such as empty matrices.

#+CAPTION: Types of the algorithm's steps
#+BEGIN_SRC idris
step1 : HungarianMatrix (S n) -> HungarianMatrix (S n)

step2 : HungarianMatrix (S n) -> HungarianMatrix (S n)

export
hungarianMethod : HungarianMatrix (S n) -> HungarianMatrix (S n)
#+END_SRC

Although we have now introduced an implementation of the algorithm which passes
the Idris totality checker by encoding the size of our cost matrix in the type
we have yet to provide any meaningful correctness properties for the steps of
the algorithm. We would like to be able to reason about the algorithm in terms
of the invariants mentioned before.

*** Third Implementation - Vectors with Proofs
In the third implementation we will build upon the previous attempt using
vectors. In this implementation however, we will create proofs about the code
that we write and enforce the invariants discussed previously. If we want to be
more certain about the correctness of the code that we write we will need to be
able to construct correct proofs that type check but are also semantically
correct in that the type is a correct specification for the proof.

The invariants discussed previously have something in common. They both express
that there must exist a zero somewhere in our matrix. The Idris standard library
provides a type =Elem= which encodes exactly this notion of existence within a
vector. There are two constructors of this type, =Here= and =There=. =Here= represents
a proof that the element at the head of the vector is equal to the element that
is being shown to exist. The repeated use of the name =x= ensures that this
reflexive equality must hold in order to construct a =Here= value. =There=
represents the fact that if an item is an element somewhere later in the vector
then it is an element of a vector constructed by prepending an element to the
front of the vector in discussion.

#+CAPTION: The Idris standard library definition of =Elem=
#+BEGIN_SRC idris
||| A proof that some element is found in a vector
data Elem : a -> Vect k a -> Type where
     Here : Elem x (x::xs)
     There : (later : Elem x xs) -> Elem x (y::xs)
#+END_SRC

This type allows us to express notions such as =(xs : Vect n a) -> Elem 0
(subSmallest xs)=. In our invariants however we want to encode that all of the
rows and columns contain a specific element. Idris also provides some
quantifiers that act over lists and vectors. These quantifiers =Any= and =All=
encode notions of existential quantification and universal quantification
respectively. If we look at how =All= is defined we can see that it is very
closely related to the =Vect= type we have been using. In a sense we can reason
about it as a vector of proofs that some property holds corresponding
element-wise to the =Vect= passed as an input to the type.

#+CAPTION: The Idris standard library definition of =All=
#+BEGIN_SRC idris
||| A proof that all elements of a vector satisfy a property. It is a list of
||| proofs, corresponding element-wise to the `Vect`.
data All : (P : a -> Type) -> Vect n a -> Type where
  Nil : {P : a -> Type} -> All P Nil
  (::) : {P : a -> Type} -> {xs : Vect n a} -> P x -> All P xs -> All P (x :: xs)
#+END_SRC

We can implement a map-like operation that takes a vector and a function that
shows the property holds for any particular element of that vector and produce a
proof that the property holds for all elements of that vector.

#+CAPTION: A proof mapping function to create a proof of =All=
#+BEGIN_SRC idris
proofMap : {P : a -> Type} -> ((x : a) -> P x) -> (xs : Vect n a) -> All P xs
proofMap _ [] = []
proofMap f (x :: xs) = f x :: proofMap f xs
#+END_SRC

Having looked at =Elem=, =All= and discussed some ways of creating proofs over
vectors and matrices we can now encode the invariants we outlined previously as
Idris types.

#+CAPTION: An encoding of our invariants as Idris types
#+BEGIN_SRC idris
step1 : HungarianMatrix (S n) -> (ys : HungarianMatrix (S n) ** All (Elem 0) ys)

step2 : (xs : HungarianMatrix (S n) ** All (Elem 0) xs)
     -> (ys : HungarianMatrix (S n) ** All (Elem 0) ys)
#+END_SRC

The =step1= function says that we will take some non-empty matrix and produce not
only a transformed matrix after having performed the operations of the algorithm
but also a proof that all of the rows of this matrix contain a zero somewhere in
them. The =step2= function takes the result of the =step1= function and produces a
newly transformed matrix and also a proof that the columns of this new matrix
all contain a zero. In both of these functions we make use of the dependent pair
type, =**=. This is Idris' encoding of Martin-Löf's \Sigma-types. In both of these
examples the /type/ of the second element of the pair is dependent upon the /value/
of the first.

We have also encoded some sense of ordering of the steps of the algorithm in
these types. We do not want to write correct implementations of the steps of the
algorithm and then interleave them in an incorrect order. By enforcing in step 2
that we first have a value that is of the type output from step 1 then we can
enforce an ordering between the steps of the algorithm. There should be no way
to call into step 2 without having obtained the value from having called step 1.
We can also think of this in terms of preconditions and postconditions. The
precondition of step 2 of the algorithm is the same as the postcondition of
step 1. This can continue for the other steps of the algorithm to ensure a
correct order of application.

There are a number of underlying proof steps that are required to get to the
point where we can write the functions =step1= and =step2= above.
1. The =vectMin= function produces a value present in the passed vector
2. Subtracting that value across a row/column produces at least one zero
3. Subtracting minimums across all rows/columns produces zeroes in all
   rows/columns

We will focus on this first proof step as an example to illustrate the approach
to performing these proofs in the type-driven style.

#+CAPTION: Proof that =vectMin= produces an element of the passed vector
#+BEGIN_SRC idris
||| If we have a function to show `x` being in `zs` implies `x` being in `as`
||| and we can show `x` is in `y :: zs` then we can show `x` is in `y :: as`
congElem : (Elem x zs -> Elem x as) -> Elem x (y :: zs) -> Elem x (y :: as)
congElem _ Here = Here
congElem f (There later) = There (f later)

vectMinElem' : Ord a => (x : a) -> (ys : Vect n a) -> Elem (vectMin' x ys) (x :: ys)
vectMinElem' x [] = Here
vectMinElem' x (y :: ys) with (compare x y)
  vectMinElem' x (y :: ys) | GT = There (vectMinElem' y ys)
  vectMinElem' x (y :: ys) | EQ = There (vectMinElem' y ys)
  vectMinElem' x (y :: ys) | LT = congElem There (vectMinElem' x ys)

vectMinElem : Ord a => (xs : Vect (S n) a) -> Elem (vectMin xs) xs
vectMinElem (x :: xs) = vectMinElem' x xs
#+END_SRC

Structurally, this proof is very similar to the definitions of =vectMin= and
=vectMin'= discussed in the previous section. We can see that the proof is
deferred to a helper function =vectMinElem'= which states that if we have value =x=
and a vector =ys= then we can prove that applying the helper function =vectMin'= to
those arguments results in a value in =x :: ys=. For the empty vector, type-driven
development allows the compiler to know that the only case that is possible is
that the element must be at the current position as we have the singleton list
=[x]= in our type. In this case proof search is sufficient to find the correct
value for us.

The use of the =with= pattern for a non-empty vector allows us to dependently
pattern match on the result of the comparison. If the value at the head of the
vector is either greater than or equal to the current minimum, by the definition
of the =vectMin'= function, we know that the value returned will not be =x= but some
later value and so the constructor =There= is used with a recursive call to
=vectMinElem'=. The =congElem= function represents the congruence of =Elem= in that if
we have an implication that =x= is in =zs= implies =x= being in =as= then if =x= is in =y
:: zs= that will imply that =x= is in =y :: as=. We use this to prove that our
minimum is in the vector when it continues to be the minimum value inspected so
far while recursing through the vector. This gives a flavour of the approach
used when writing proofs of functions in Idris. The rest of the proofs for the
steps of the algorithm are provided as part of the full code listing for the
=HungarianMatrixProof= module should you wish to inspect more of these same
approaches at work.

There were some particular challenges that appeared while writing this
implementation. The first of which was the lack of dependent pattern matching
for integers. As =Int= is defined as a primitive type in Idris we cannot perform
any meaningful induction over its structure like we can do with the =Nat=
datatype. As part of this proven implementation we need to be able to show that
subtracting a number from itself produces zero. If our implementation were using
natural numbers then we could leverage the already defined proof of this fact
from the standard library. Unfortunately the Hungarian algorithm has to deal
with negative numbers in the later steps of the algorithm and use of =Nat= won't
be satisfactory for an implementation. As we can't do dependent pattern matching
on =Int= we have to define this is a postulate that we tell Idris to trust. For
small and self-evident theorems such as this it may be acceptable to write a
postulate but we would like to encode as much of our proof within the Idris
framework as possible so having to use postulates is indicative of a problem in
the code. There exists a wrapper around =Nat= to represent signed integers in the
standard library however it was only discovered after the version using integers
was complete.

#+CAPTION: We tell Idris to trust us that $x\mathbin{-} x \mathrel{=} 0$
#+BEGIN_SRC idris
postulate minusSelfZero : {x : Int} -> x - x = 0
#+END_SRC

Another pain point encountered were the sometimes complex type errors. In one
instance a particularly wordy and incomprehensible 200 line type error was
produced when attempting to prove that all of the rows contained zeroes. The use
of the names =meth1=, =meth5= etc. indicated that we needed to somehow examine how
interfaces were being used in our implementation. The problem turned out to only
demonstrate itself in a particular use of the =map= function from the =Functor=
interface. Replacing the call to =map= with a local definition specialised only to
work over =Vect= caused the type errors to disappear. This turned out to be a bug
in the Idris compiler and how it handles interfaces that has since been
corrected although we weren't aware of this at the time of writing the code.
These cryptic error messages can often manifest themselves when working in a
system with such an expressive type system. It will often be hard to tell if the
error is due to a mistake on your part or if it is some detail of a bug in the
compiler leaking out into your code.

*** API Considerations
It is worth considering what kind of API we wish to expose to the consumers of
our algorithm. Although we have these complicated proof terms as part of our
module we can make some decisions about how simple or complicated the API is to
use. In each implementation we have elected to export a simple function that
performs the steps of the algorithm. Each of our modules contains a definition
similar to the following.

#+CAPTION: Our user-facing API for the Hungarian algorithm
#+BEGIN_SRC idris
export
hungarianMethod : HungarianMatrix (S n) -> HungarianMatrix (S n)
hungarianMethod xs = transpose (fst (step2 (step1 xs)))
#+END_SRC

With dependent types we can choose to expose as much or as little of the
underlying proof terms as we wish. We may not wish to complicate the use of our
library with the correctness proofs that ensure that we have a rigorous
implementation that is formally verified. We can achieve a simple user-facing
API while still having that formalism beneath ensuring that more static errors
are detected at compile time. To compliment this simple API a number of other
correctness proofs about the main functions could be exported from the module
separately. If someone wanted to take advantage of the fact the implementation
has been verified then they can use these properties in their code. For most
consumers of the library this will probably not be the case but it is still
possible to satisfy both the users that want a simple API and those that would
like access to the underlying formalism that ensures that the simple API behaves
as it should.

*** Haskell Port - Bottom Up Approach
Another implementation approach taken when tackling this algorithm was to take
an existing Haskell version of the Hungarian method and port it to Idris. From
there, we could add further specification of the correctness of our
implementation to the types of the ported functions. This can be seen as more of
a bottom-up approach as compared to the top-down approach of type-driven
development where the specification (types) come first.

The code being ported was a package cite:komuves_munkres_2008 on the Haskell
package server Hackage. The aim was to take this package and directly port the
code over to Idris however there were a number of complications with this
approach. The major problem was the use of efficient stateful data structures
such as =Data.Array= in the Haskell package. Idris doesn't provide an analogous
data structure to this efficient array type. The Haskell implementation had to
be pulled apart in order to translate it into a representation using lists and
recursion rather than mutating references to arrays. The surface syntax may make
it seem simple to port this code to Idris however that wasn't the case as we
found. There were some subtleties involved in getting the code to type-check in
Idris.

At this point we can begin to add our correctness proofs onto our ported Idris
code. Ultimately this work had to be abandoned and a new approach taken in the
type-driven style. The implementation wasn't suited to writing proofs at least
not in a simple manner. Much more benefit and success was had when writing code
that was specifically set out to be proven in the first place. This means
specifying the properties you want to prove at the outset in your types and by
implementing the functions in a type-driven development fashion.

** Run-Length Encoding
The next case study we'll examine is an Idris implementation of run-length
encoding.

*** Problem Definition
Numerous compression algorithms and techniques exist to squeeze data into
occupying fewer bytes in memory or on disk. Often these algorithms optimise for
specific use cases. For example, the compression present in the JPEG image
format is lossy to because of the visual nature of the data being compressed.
Even with mild loss to the quality of the image the overall image will still be
only slightly distinguishable from the original image. Lossy compression schemes
also exist for audio and video data again to optimise data that is tolerant to a
loss of quality. For text however we don't want to introduce lossy compression
as that would distort the original meaning of the data perceivably.

Run-length encoding is a simple technique used to compress text data. The data
is encoded as a series of ``runs'' which represent multiple repeated occurrences
of a single character. To illustrate the working of this compression scheme the
string ``foooobaaaar'' would be encoded as ``1f4o1b4a1r''. There is one 'f'
followed by four 'o's and so on. If we were to store the run counters in 8 bit
unsigned integers and deal with 8 bit ASCII characters the compressed version
would come in at 10 bytes versus the original 11 bytes. This algorithm has saved
very little memory in this example but we can see how this might scale up for
longer runs in larger strings.

*** Invariants
There are two invariants that we might want to prove about a compression scheme
such as run-length encoding. The first of which is that the compression
algorithm actually produces a value which occupies less space than the original
value. The second invariant is that there exists an identity from composing
compress and decompress. If we take some data and call a compression function on
it and then pass that result on to a decompression function then we would expect
to get back the same initial data that we compressed. In this sense =decompress
(compress data)= should be equal to =id data= and as a result =decompress . compress=
should equal =id=. For reasons outlined later we will see why both of these
invariants are either quite difficult or impossible to encode and prove.

*** Idris Implementation
This case study began by taking an initial piece of example code from the Idris
repository that performed a simple run-length encoding. The initial code
demonstrated how a run-length encoder might be written but did not properly
encode the data. The output of the function was a string which does not
accurately represent how we want to encode the data. A proper encoder would
alternate between an 8 bit integer and an 8 bit ASCII character to produce an
efficient format capable of being written byte-by-byte to a file.

#+CAPTION: Intermediate format encoder for run-length encoding
#+BEGIN_SRC idris
intermediate : {auto p : m `LTE` (S n)} -> Vect (S n) Char -> (m : Nat ** Vect (S m) (Nat, Char))
intermediate xs with (rle xs)
  intermediate (_ :: _) | REnd impossible
  intermediate (c :: (replicate n c ++ [])) | (RChar n c rs) = (_ ** [(S n, c)])
  intermediate (c :: (replicate n c ++ (z :: zs))) | (RChar n c rs)
    = let (_ ** ws) = intermediate (z :: zs)
        in (_ ** (S n, c) :: ws)
#+END_SRC

This function creates an intermediate representation of our run-length encoded
data as a vector of pairs of natural numbers and characters. The natural numbers
give the length of the run and the character represents the character that is
being counted for that run. As seen before, we make use of the dependent pair
type =**= to specify the length of our resultant vector. We also make use of =auto=
proof search to ensure that the length of the resultant vector is less than or
equal to the length of the vector of characters passed in to be compressed. This
should be the case as if we have all length 1 runs then the vectors will have
equal length and if there is ever a run that is more than 1 long then there will
be fewer intermediate values as we only need to store the character once with
its run length.

With our intermediate value representation we can now begin to turn the data
into bytes that are suitable for output to a file. Idris includes a library
=Data.Bits= that contains numerous functions for manipulating binary data. We can
represent the data that we want to flush out to our file as a list of =Bits 8=
values which essentially map down to bytes. The drawback of this approach is
that we will truncate runs longer than 255 as we only have 8 bits to work with.
An extension to this scheme could be added such that runs longer than 255 are
represented as separate runs.

#+CAPTION: Translation of our intermediate data to bytes
#+BEGIN_SRC idris
compressedBits : (n : Nat ** Vect (S n) (Nat, Char)) -> List (Bits 8)
compressedBits (Z ** ((n, c) :: [])) = [intToBits (cast n), intToBits (cast (ord c))]
compressedBits ((S x) ** ((n, c) :: xs)) = intToBits (cast n) :: intToBits (cast (ord c)) :: compressedBits (x ** xs)
#+END_SRC

With the information encoded as bytes we can use the Idris =Data.Buffer= module to
allocate a buffer that we can write the data into. We know that the size of the
buffer we want to allocate will be 2 times the length of our intermediate vector
format. One byte will be allocated for the length of the run and one byte
allocated for the actual character. Recursion over our list of bytes allows us
to build up a series of =IO= actions for writing out to our allocated buffer. We
can then sequence these actions to perform the modifications to the buffer and
flush it out to a file as compressed data.

#+CAPTION: The main RLE program
#+BEGIN_SRC idris
writeCompressed' : List (Bits 8) -> Buffer -> Int -> List (IO ())
writeCompressed' [] buf loc = [pure ()]
writeCompressed' ((MkBits x) :: xs) buf loc = setByte buf loc x :: writeCompressed' xs buf (loc + 1)

writeCompressed : List (Bits 8) -> Buffer -> List (IO ())
writeCompressed xs buf = writeCompressed' xs buf 0

main : IO ()
main = do
  putStrLn ("Compressing: " ++ testString)
  putStrLn $ show compressed
  let bufferSize = cast $ 2 * (S (fst inter))
  Just buffer <- newBuffer bufferSize
    | Nothing => putStrLn "Failed to allocate buffer" -- If out of memory
  sequence_ $ writeCompressed compressed buffer -- Fill buffer
  Right file <- openFile "output.bin" WriteTruncate -- Get file handle
    | Left _ => putStrLn "Failed to get file handle"
  writeBufferToFile file buffer bufferSize
  putStrLn "Output written to output.bin"
#+END_SRC

*** Proving Our Invariants
